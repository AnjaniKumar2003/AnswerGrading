{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m_YzhQgPuSQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math as mt\n",
        "import random \n",
        "import copy as cp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive' , force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuW2126mPyIJ",
        "outputId": "4b52cd56-1349-43a8-be0a-e0e10dfc0f1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Project/train.tsv\" , sep= '\\t')\n",
        "mod_data = pd.DataFrame({\"Id\":[] , \"EssaySet\" :[] , \"Score1\" : [] , \"Score2\" : [] , \"EssayText\" : []})\n",
        "for i in range(1,3):\n",
        "  k = data.loc[data[\"EssaySet\"] == i]\n",
        "  mod_data = pd.concat([mod_data,k])\n",
        "mod_data.describe()\n",
        "scores1 = np.array(mod_data['Score1'].tolist())\n",
        "scores2 = np.array(mod_data['Score2'].tolist())\n",
        "Eset = np.array(mod_data['EssaySet'].tolist()).reshape(-1,1)\n",
        "scores = (scores1 + scores2)/2\n",
        "scores = scores.reshape(-1,1)\n",
        "labels = np.concatenate((scores, Eset) , axis =1)"
      ],
      "metadata": {
        "id": "V_JKjWH3Pz_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load vectors Here\n",
        "#=======Load Vectors==================#\n",
        "\n",
        "inputs_all = np.load('/content/drive/MyDrive/Project/all_vecFile_12 _final.npy')\n",
        "print(inputs_all.shape)\n",
        "# predicting the scores\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(inputs_all, labels , test_size=0.2, random_state=33)\n",
        "\n",
        "X_train = torch.Tensor(X_train)\n",
        "X_test = torch.Tensor(X_test)\n",
        "Y_train = torch.Tensor(Y_train)\n",
        "Y_test = torch.Tensor(Y_test)\n",
        "\n",
        "train = TensorDataset(X_train , Y_train)\n",
        "test = TensorDataset(X_test , Y_test)\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=20, shuffle=False)\n",
        "test_loader = DataLoader(test, batch_size=20, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON5JaeucQVWg",
        "outputId": "7b62a643-05c2-42e6-aea7-9f02eed9164b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2950, 42, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/janfreyberg/pytorch-revgrad.git"
      ],
      "metadata": {
        "id": "CVu-z8YPQfN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt \n",
        "from pytorch_revgrad import RevGrad\n",
        "\n",
        "# loading local VRNN hidden vectors\n",
        "hidden_locals = np.load(\"/content/drive/MyDrive/Project/hiddens_final.npy\")\n",
        "\n",
        "# changing device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EPS = torch.finfo(torch.float).eps # numerical logs\n",
        "hidden_locals = torch.from_numpy(hidden_locals)\n",
        "\n",
        "\n",
        "class VRNN(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim, n_layers, output_dims, domains ,  bias=False):\n",
        "        super(VRNN, self).__init__()\n",
        "\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.domains = domains\n",
        "        self.output_dims = output_dims\n",
        "\n",
        "        #feature-extracting transformations\n",
        "        self.phi_x = nn.Sequential(\n",
        "            nn.Linear(x_dim, h_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(h_dim, h_dim),\n",
        "            nn.ReLU())\n",
        "        self.phi_z = nn.Sequential(\n",
        "            nn.Linear(z_dim, h_dim),\n",
        "            nn.ReLU())\n",
        "\n",
        "        #encoder\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Linear(h_dim + h_dim, h_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(h_dim, h_dim),\n",
        "            nn.ReLU())\n",
        "        self.enc_mean = nn.Linear(h_dim, z_dim)\n",
        "        self.enc_std = nn.Sequential(\n",
        "            nn.Linear(h_dim, z_dim),\n",
        "            nn.Softplus())\n",
        "\n",
        "        #prior\n",
        "        self.prior = nn.Sequential(\n",
        "            nn.Linear(h_dim, h_dim),\n",
        "            nn.ReLU())\n",
        "        self.prior_mean = nn.Linear(h_dim, z_dim)\n",
        "        self.prior_std = nn.Sequential(\n",
        "            nn.Linear(h_dim, z_dim),\n",
        "            nn.Softplus())\n",
        "\n",
        "        #decoder\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Linear(h_dim + h_dim, h_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(h_dim, h_dim),\n",
        "            nn.ReLU())\n",
        "        self.dec_std = nn.Sequential(\n",
        "            nn.Linear(h_dim, x_dim),\n",
        "            nn.Softplus())\n",
        "        #self.dec_mean = nn.Linear(h_dim, x_dim)\n",
        "        self.dec_mean = nn.Sequential(\n",
        "            nn.Linear(h_dim, x_dim),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        #recurrence\n",
        "        self.rnn = nn.LSTM(h_dim + h_dim, h_dim, n_layers, bias)\n",
        "\n",
        "        # Classifier \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(h_dim, output_dims),\n",
        "            nn.Softmax(dim=1))\n",
        "\n",
        "        # Domain Disciminator\n",
        "        self.discriminator = nn.Sequential(\n",
        "            nn.Linear(h_dim , domains),\n",
        "            nn.Softmax(dim=1),\n",
        "            RevGrad())\n",
        "\n",
        "    def forward(self, x ):\n",
        "\n",
        "        all_enc_mean, all_enc_std = [], []\n",
        "        all_dec_mean, all_dec_std = [], []\n",
        "        kld_loss = 0\n",
        "        nll_loss = 0\n",
        "        out = []\n",
        "\n",
        "        h = torch.zeros(self.n_layers, x.size(1), self.h_dim, device=device)\n",
        "        for t in range(x.size(0)):\n",
        "            phi_x_t = self.phi_x(x[t])\n",
        "\n",
        "            #encoder\n",
        "            enc_t = self.enc(torch.cat([phi_x_t, h[-1]], 1))\n",
        "            enc_mean_t = self.enc_mean(enc_t)\n",
        "            enc_std_t = self.enc_std(enc_t) \n",
        "            #prior\n",
        "            prior_t = self.prior(h[-1])\n",
        "            prior_mean_t = self.prior_mean(prior_t)\n",
        "            prior_std_t = self.prior_std(prior_t)\n",
        "\n",
        "            #sampling and reparameterization\n",
        "            z_t = self._reparameterized_sample(enc_mean_t, enc_std_t)\n",
        "            phi_z_t = self.phi_z(z_t)\n",
        "\n",
        "            #decoder\n",
        "            dec_t = self.dec(torch.cat([phi_z_t, h[-1]], 1))\n",
        "            dec_mean_t = self.dec_mean(dec_t)\n",
        "            dec_std_t = self.dec_std(dec_t)\n",
        "\n",
        "            #recurrence\n",
        "            out , h = self.rnn(torch.cat([phi_x_t, phi_z_t], 1).unsqueeze(0), h)\n",
        "\n",
        "            #computing losses\n",
        "            kld_loss += self._kld_gauss(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n",
        "            #nll_loss += self._nll_gauss(dec_mean_t, dec_std_t, x[t])\n",
        "            nll_loss += self._nll_bernoulli(dec_mean_t, x[t])\n",
        "\n",
        "        # Computing Diff Loss\n",
        "        #taking the last hidden layer and computing l2 norm with the local VRNN hiddens\n",
        "        fin_hidden = h[:,-1,:]\n",
        "        diff_loss = float(torch.sum(fin_hidden * hidden_locals))\n",
        "\n",
        "\n",
        "        # The last hidden computed layer , will be utilized for domain discriminator and domain classifier\n",
        "        class_vector = self.classifier(out)[0]\n",
        "        domain_vector = self.discriminator(out)[0]\n",
        "\n",
        "        return kld_loss, nll_loss, h , class_vector.data , domain_vector.data , diff_loss ,\\\n",
        "            (all_enc_mean, all_enc_std), \\\n",
        "            (all_dec_mean, all_dec_std)\n",
        "\n",
        "\n",
        "    def sample(self, seq_len):\n",
        "\n",
        "        sample = torch.zeros(seq_len, self.x_dim, device=device)\n",
        "\n",
        "        h = torch.zeros(self.n_layers, 1, self.h_dim, device=device)\n",
        "        for t in range(seq_len):\n",
        "\n",
        "            #prior\n",
        "            prior_t = self.prior(h[-1])\n",
        "            prior_mean_t = self.prior_mean(prior_t)\n",
        "            prior_std_t = self.prior_std(prior_t)\n",
        "\n",
        "            #sampling and reparameterization\n",
        "            z_t = self._reparameterized_sample(prior_mean_t, prior_std_t)\n",
        "            phi_z_t = self.phi_z(z_t)\n",
        "\n",
        "            #decoder\n",
        "            dec_t = self.dec(torch.cat([phi_z_t, h[-1]], 1))\n",
        "            dec_mean_t = self.dec_mean(dec_t)\n",
        "            #dec_std_t = self.dec_std(dec_t)\n",
        "\n",
        "            phi_x_t = self.phi_x(dec_mean_t)\n",
        "\n",
        "            #recurrence\n",
        "            _, h = self.rnn(torch.cat([phi_x_t, phi_z_t], 1).unsqueeze(0), h)\n",
        "\n",
        "            sample[t] = dec_mean_t.data\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "    def reset_parameters(self, stdv=1e-1):\n",
        "        for weight in self.parameters():\n",
        "            weight.data.normal_(0, stdv)\n",
        "\n",
        "\n",
        "    def _init_weights(self, stdv):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def _reparameterized_sample(self, mean, std):\n",
        "        \"\"\"using std to sample\"\"\"\n",
        "        eps = torch.empty(size=std.size(), device=device, dtype=torch.float).normal_()\n",
        "        return eps.mul(std).add_(mean)\n",
        "\n",
        "\n",
        "    def _kld_gauss(self, mean_1, std_1, mean_2, std_2):\n",
        "        \"\"\"Using std to compute KLD\"\"\"\n",
        "\n",
        "        kld_element =  (2 * torch.log(std_2 + EPS) - 2 * torch.log(std_1 + EPS) + \n",
        "            (std_1.pow(2) + (mean_1 - mean_2).pow(2)) /\n",
        "            std_2.pow(2) - 1)\n",
        "        return\t0.5 * torch.sum(kld_element)\n",
        "\n",
        "\n",
        "    def _nll_bernoulli(self, theta, x):\n",
        "        return - torch.sum(x*torch.log(theta + EPS) + (1-x)*torch.log(1-theta-EPS))\n",
        "\n",
        "\n",
        "    def _nll_gauss(self, mean, std, x):\n",
        "        return torch.sum(torch.log(std + EPS) + torch.log(2*torch.pi)/2 + (x - mean).pow(2)/(2*std.pow(2)))"
      ],
      "metadata": {
        "id": "0iyYBVvPQfkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_dim = 300\n",
        "h_dim = 2*50   # 2 Essays , each considered as one vector\n",
        "z_dim = 10\n",
        "n_layers = 1 \n",
        "n_epochs = 100\n",
        "clip = 10\n",
        "learning_rate = 0.0081\n",
        "seed = 128\n",
        "print_every = 10 # batches\n",
        "save_every = 1 # epochs\n",
        "domains = 2\n",
        "classes = len(np.unique(scores))\n",
        "batch_size = 20\n",
        "\n"
      ],
      "metadata": {
        "id": "CypL4rd_R2Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('/content/drive/MyDrive/Project/weights')\n",
        "model = VRNN(x_dim, h_dim, z_dim ,n_layers, classes ,domains)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UFgvgTVRXPp",
        "outputId": "77196082-38b6-4798-e379-178cf5fec740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i,data in enumerate(test_loader):\n",
        "        \n",
        "        data = test[0][0]\n",
        "        class_labels = test[1][0]\n",
        "        domain_labels = test[1][1]\n",
        "        data = data.squeeze().transpose(0, 1)\n",
        "        data = (data - data.min()) / (data.max() - data.min())\n",
        "        kld_loss, nll_loss,h, class_vector , domain_vector ,  diff_loss , _, _ = model(data)\n",
        "        \n",
        "        print(f'{i+1}.)    {str(class_vector.argmax().item())}     {class_labels}')\n",
        "        \n",
        "        if(class_vector.argmax().item() == Y_test[i]):\n",
        "            correct+=1\n",
        "            \n",
        "print(f'We got {correct} correct')"
      ],
      "metadata": {
        "id": "klQ4Yb30RV7B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}