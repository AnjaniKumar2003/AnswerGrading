{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2eec23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "\"\"\"implementation of the Variational Recurrent\n",
    "Neural Network (VRNN) from https://arxiv.org/abs/1506.02216\n",
    "using unimodal isotropic gaussian distributions for \n",
    "inference, prior, and generating models.\"\"\"\n",
    "\n",
    "# changing device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPS = torch.finfo(torch.float).eps # numerical logs\n",
    "\n",
    "class VRNN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, z_dim, n_layers, bias=False):\n",
    "        super(VRNN, self).__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #feature-extracting transformations\n",
    "        self.phi_x = nn.Sequential(\n",
    "            nn.Linear(x_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU())\n",
    "        self.phi_z = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU())\n",
    "\n",
    "        #encoder\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(h_dim + h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU())\n",
    "        self.enc_mean = nn.Linear(h_dim, z_dim)\n",
    "        self.enc_std = nn.Sequential(\n",
    "            nn.Linear(h_dim, z_dim),\n",
    "            nn.Softplus())\n",
    "\n",
    "        #prior\n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU())\n",
    "        self.prior_mean = nn.Linear(h_dim, z_dim)\n",
    "        self.prior_std = nn.Sequential(\n",
    "            nn.Linear(h_dim, z_dim),\n",
    "            nn.Softplus())\n",
    "\n",
    "        #decoder\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(h_dim + h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU())\n",
    "        self.dec_std = nn.Sequential(\n",
    "            nn.Linear(h_dim, x_dim),\n",
    "            nn.Softplus())\n",
    "        #self.dec_mean = nn.Linear(h_dim, x_dim)\n",
    "        self.dec_mean = nn.Sequential(\n",
    "            nn.Linear(h_dim, x_dim),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        #recurrence\n",
    "        self.rnn = nn.GRU(h_dim + h_dim, h_dim, n_layers, bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        all_enc_mean, all_enc_std = [], []\n",
    "        all_dec_mean, all_dec_std = [], []\n",
    "        kld_loss = 0\n",
    "        nll_loss = 0\n",
    "\n",
    "        h = torch.zeros(self.n_layers, x.size(1), self.h_dim, device=device)\n",
    "        for t in range(x.size(0)):\n",
    "\n",
    "            phi_x_t = self.phi_x(x[t])\n",
    "\n",
    "            #encoder\n",
    "            enc_t = self.enc(torch.cat([phi_x_t, h[-1]], 1))\n",
    "            enc_mean_t = self.enc_mean(enc_t)\n",
    "            enc_std_t = self.enc_std(enc_t) \n",
    "\n",
    "            #prior\n",
    "            prior_t = self.prior(h[-1])\n",
    "            prior_mean_t = self.prior_mean(prior_t)\n",
    "            prior_std_t = self.prior_std(prior_t)\n",
    "\n",
    "            #sampling and reparameterization\n",
    "            z_t = self._reparameterized_sample(enc_mean_t, enc_std_t)\n",
    "            phi_z_t = self.phi_z(z_t)\n",
    "\n",
    "            #decoder\n",
    "            dec_t = self.dec(torch.cat([phi_z_t, h[-1]], 1))\n",
    "            dec_mean_t = self.dec_mean(dec_t)\n",
    "            dec_std_t = self.dec_std(dec_t)\n",
    "\n",
    "            #recurrence\n",
    "            _, h = self.rnn(torch.cat([phi_x_t, phi_z_t], 1).unsqueeze(0), h)\n",
    "\n",
    "            #computing losses\n",
    "            kld_loss += self._kld_gauss(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n",
    "            #nll_loss += self._nll_gauss(dec_mean_t, dec_std_t, x[t])\n",
    "            nll_loss += self._nll_bernoulli(dec_mean_t, x[t])\n",
    "\n",
    "            all_enc_std.append(enc_std_t)\n",
    "            all_enc_mean.append(enc_mean_t)\n",
    "            all_dec_mean.append(dec_mean_t)\n",
    "            all_dec_std.append(dec_std_t)\n",
    "\n",
    "        return kld_loss, nll_loss, \\\n",
    "            (all_enc_mean, all_enc_std), \\\n",
    "            (all_dec_mean, all_dec_std)\n",
    "\n",
    "\n",
    "    def sample(self, seq_len):\n",
    "\n",
    "        sample = torch.zeros(seq_len, self.x_dim, device=device)\n",
    "\n",
    "        h = torch.zeros(self.n_layers, 1, self.h_dim, device=device)\n",
    "        for t in range(seq_len):\n",
    "\n",
    "            #prior\n",
    "            prior_t = self.prior(h[-1])\n",
    "            prior_mean_t = self.prior_mean(prior_t)\n",
    "            prior_std_t = self.prior_std(prior_t)\n",
    "\n",
    "            #sampling and reparameterization\n",
    "            z_t = self._reparameterized_sample(prior_mean_t, prior_std_t)\n",
    "            phi_z_t = self.phi_z(z_t)\n",
    "\n",
    "            #decoder\n",
    "            dec_t = self.dec(torch.cat([phi_z_t, h[-1]], 1))\n",
    "            dec_mean_t = self.dec_mean(dec_t)\n",
    "            #dec_std_t = self.dec_std(dec_t)\n",
    "\n",
    "            phi_x_t = self.phi_x(dec_mean_t)\n",
    "\n",
    "            #recurrence\n",
    "            _, h = self.rnn(torch.cat([phi_x_t, phi_z_t], 1).unsqueeze(0), h)\n",
    "\n",
    "            sample[t] = dec_mean_t.data\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def reset_parameters(self, stdv=1e-1):\n",
    "        for weight in self.parameters():\n",
    "            weight.data.normal_(0, stdv)\n",
    "\n",
    "\n",
    "    def _init_weights(self, stdv):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _reparameterized_sample(self, mean, std):\n",
    "        \"\"\"using std to sample\"\"\"\n",
    "        eps = torch.empty(size=std.size(), device=device, dtype=torch.float).normal_()\n",
    "        return eps.mul(std).add_(mean)\n",
    "\n",
    "\n",
    "    def _kld_gauss(self, mean_1, std_1, mean_2, std_2):\n",
    "        \"\"\"Using std to compute KLD\"\"\"\n",
    "\n",
    "        kld_element =  (2 * torch.log(std_2 + EPS) - 2 * torch.log(std_1 + EPS) + \n",
    "            (std_1.pow(2) + (mean_1 - mean_2).pow(2)) /\n",
    "            std_2.pow(2) - 1)\n",
    "        return\t0.5 * torch.sum(kld_element)\n",
    "\n",
    "\n",
    "    def _nll_bernoulli(self, theta, x):\n",
    "        return - torch.sum(x*torch.log(theta + EPS) + (1-x)*torch.log(1-theta-EPS))\n",
    "\n",
    "\n",
    "    def _nll_gauss(self, mean, std, x):\n",
    "        return torch.sum(torch.log(std + EPS) + torch.log(2*torch.pi)/2 + (x - mean).pow(2)/(2*std.pow(2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ef33b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
